# -*- coding: utf-8 -*-
"""Churning_customers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZQsAf0JNprzvnEJiXl5QQ-BrBcB0eHPm

# Data Reading and Processing

*Importing necessary libraries and resources that will be needed during the execution of the program.*
"""

!pip install scikeras

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from scikeras.wrappers import KerasClassifier, KerasRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Model
from keras.layers import Input, Dense
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')

"""*Loading the data sets that will be used in training, testing and validating i.e CustomerChurn_dataset.csv*"""

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Customer Churning/CustomerChurn_dataset.csv')

"""*Inspecting the initial rows of our DataFrame. This allows me to see the first few rows of our dataset, helping me understand the structure of the data and the types of values present in each column. It gives me a sense of what the data looks like and will help me decide on further analysis and preprocessing steps.*"""

df

msno.matrix(df);

df.describe()

df.info()

"""**Dropping uneccessary columns in the dataframe**

*Reason for dropping*


*customerID contains unique identifiers for each customer. In predictive modeling, unique identifiers are generally not useful as features because they do not contribute to predicting the target variable (Churn in this case). Including such identifiers can potentially lead the model to overfit to specific individuals rather than learning general patterns.*
"""

columns_to_drop = ['customerID']
df = df.drop(columns=columns_to_drop, axis=1)

"""**Label encoding categorical columns**"""

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Apply Label Encoding to categorical features
for column in df.columns:
    if df[column].dtype == 'object':
        df[column] = label_encoder.fit_transform(df[column])

"""**Displaying the data after encoding**"""

df.info()

df

"""*Preparing the data.  X contains the features, and y contains the corresponding labels (target variable).*"""

X = df.drop('Churn', axis=1)
y = df['Churn']

"""FEATURE SELECTION

*The below code is creating an instance of the RandomForestClassifier class and fitting it to the training data X and y. This means that it is training the random forest classifier on the given input features X and their corresponding target labels y.*
"""

rf_classifier = RandomForestClassifier()
rf_classifier.fit(X, y)

"""*The code below is calculating the feature importances of a random forest classifier (`rf_classifier`) and creating a dataframe (`feature_importance_df`) to store the feature names and their corresponding importances. The dataframe is then sorted in descending order based on the importance values.*"""

np.random.seed(42)
feature_importances = rf_classifier.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
feature_importance_df['Cumulative Importance'] = feature_importance_df['Importance'].cumsum()
selected_features = feature_importance_df[feature_importance_df['Cumulative Importance'] <= 0.80]['Feature']
selected_features_df = feature_importance_df[feature_importance_df['Feature'].isin(selected_features)]

"""*printing the features*

"""

print("Selected Features with Importances:")
print(selected_features_df)

"""*Assigning the selected features to X  and displaying them*"""

X = X[selected_features]
X

"""*Plotting selected feature importances*"""

plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=selected_features_df, palette='viridis')
plt.title('Selected Feature Importances')
plt.show()

"""*Plotting cumulative importance for selected features*"""

plt.figure(figsize=(12, 6))
sns.lineplot(x='Feature', y='Cumulative Importance', data=selected_features_df, color='orange')
plt.title('Cumulative Feature Importance for Selected Features')
plt.show()

"""# Exploratory data Analysis (EDA)"""

plt.figure(figsize=(12, 6))
sns.countplot(x='gender', hue='Churn', data=df, palette='viridis')
plt.title('Churn Distribution by Gender')
plt.show()

"""Senior Citizenship Analysis"""

plt.figure(figsize=(12, 6))
sns.countplot(x='SeniorCitizen', hue='Churn', data=df, palette='viridis')
plt.title('Churn Distribution by Senior Citizenship')
plt.show()

"""Partner and Dependents Analysis"""

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))
sns.countplot(x='Partner', hue='Churn', data=df, ax=axes[0], palette='viridis')
sns.countplot(x='Dependents', hue='Churn', data=df, ax=axes[1], palette='viridis')
axes[0].set_title('Churn Distribution by Partner')
axes[1].set_title('Churn Distribution by Dependents')
plt.show()

""" Service-related Analysis"""

service_cols = ['OnlineSecurity', 'TechSupport', 'InternetService']
for col in service_cols:
    plt.figure(figsize=(12, 6))
    sns.countplot(x=col, hue='Churn', data=df, palette='viridis')
    plt.title(f'Churn Distribution by {col}')
    plt.show()

"""
Contract and Payment Analysis"""

contract_cols = ['Contract', 'PaymentMethod']
for col in contract_cols:
    plt.figure(figsize=(12, 6))
    sns.countplot(x=col, hue='Churn', data=df, palette='viridis')
    plt.title(f'Churn Distribution by {col}')
    plt.show()

"""Financial Metrics Analysis

"""

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))
sns.boxplot(x='Churn', y='MonthlyCharges', data=df, ax=axes[0], palette='viridis')
sns.boxplot(x='Churn', y='TotalCharges', data=df, ax=axes[1], palette='viridis')
axes[0].set_title('Monthly Charges by Churn')
axes[1].set_title('Total Charges by Churn')
plt.show()

"""Tenure Analysis"""

plt.figure(figsize=(12, 6))
sns.histplot(x='tenure', hue='Churn', data=df, bins=30, kde=True, palette='viridis')
plt.title('Churn Distribution by Tenure')
plt.show()

"""Diverging Correlation Matrix"""

plt.figure(figsize=(20, 16))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".2f", center=0)
plt.title('Diverging Correlation Matrix')
plt.show()

"""Scatter plot of Monthly Charges against Tenure"""

plt.figure(figsize=(12, 6))
sns.scatterplot(x='MonthlyCharges', y='tenure', hue='Churn', data=df, palette='viridis')
plt.title('Scatter Plot of Monthly Charges vs. Tenure')
plt.show()

"""Scatter plot of Total Charges against Tenure"""

plt.figure(figsize=(12, 6))
sns.scatterplot(x='TotalCharges', y='tenure', hue='Churn', data=df, palette='viridis')
plt.title('Scatter Plot of Total Charges vs. Tenure')
plt.show()

"""
Checking for any class imbalance"""

plt.figure(figsize=(8, 6))
df['Churn'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['#66b3ff','#99ff99'], explode=(0.1, 0), labels=['No Churn', 'Churn'])
plt.title('Distribution of Churn Classes')
plt.show()

"""#Scaling and Splitting

The code is performing feature scaling on the data stored in the variable X. It is using the StandardScaler class from the scikit-learn library to standardize the features by removing the mean and scaling to unit variance. The fit_transform method is used to fit the scaler to the data and transform the data in a single step.
"""

scaler = StandardScaler()
X = scaler.fit_transform(X)

"""The below code is splitting the dataset into three parts: training set, validation set, and test set."""

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""Display the shapes of the resulting sets"""

print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)
print("Validation set shape:", X_val.shape, y_val.shape)

"""Plotting the distribution of classes in each set"""

plt.figure(figsize=(10, 6))
sns.countplot(x=y_train, palette='viridis')
plt.title('Distribution of Classes in Training Set')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x=y_test, palette='viridis')
plt.title('Distribution of Classes in Testing Set')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x=y_val, palette='viridis')
plt.title('Distribution of Classes in Validation Set')
plt.show()

"""# Training a Multi-Layer Perceptron model (Keras) using the Functional API

*The function creates a multi-layer perceptron (MLP) model with three hidden layers using the functional API with a specified input shape and returns the compiled model. The model used was Keras*
"""

def create_mlp_model(input_shape):
    inputs = Input(shape=(input_shape,))
    hidden1 = Dense(64, activation='relu')(inputs)
    hidden2 = Dense(32, activation='relu')(hidden1)
    hidden3 = Dense(16, activation='relu')(hidden2)
    output = Dense(1, activation='sigmoid')(hidden3)

    model = Model(inputs=inputs, outputs=output)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

"""*The below code is creating a KerasClassifier model for a multi-layer perceptron (MLP) neural network. It is using the create_mlp_model function (which makes use of a keras Model) to define the architecture of the model. The model is then passed as an argument to the KerasClassifier class, along with other parameters such as input_shape, epochs, batch_size, and verbose.*"""

mlp_model = KerasClassifier(build_fn=create_mlp_model, input_shape=X_train.shape[1], epochs=10, batch_size=32, verbose=1)
param_grid = {
    'epochs': [10, 20],
    'batch_size': [32, 64],
    'validation_split': [0.1, 0.2]
}

"""*The  code is performing a grid search to find the best hyperparameters for a multi-layer perceptron (MLP) model. It uses the GridSearchCV to search through a specified parameter grid and evaluate the model's performance using
cross-validation.*
"""

grid_search = GridSearchCV(estimator=mlp_model, param_grid=param_grid, cv=StratifiedKFold(n_splits=3), scoring='accuracy')
grid_result = grid_search.fit(X_train, y_train)

"""
*Displaying the best parameters and their corresponding accuracy*"""

print("Best parameters found: ", grid_result.best_params_)
print("Best accuracy found: {:.2%}".format(grid_result.best_score_))

"""*The code is training a multi-layer perceptron (MLP) model using the training data `X_train` and `y_train`. It creates an MLP model using the function `create_mlp_model()` and then fits the model to the training data using the specified number of epochs and batch size.*"""

best_mlp_model = create_mlp_model(X_train.shape[1])
history = best_mlp_model.fit(X_train, y_train, epochs=grid_result.best_params_['epochs'], batch_size=grid_result.best_params_['batch_size'], verbose=2)

"""*Evaluate the model on the validation set*"""

validation_accuracy = best_mlp_model.evaluate(X_val, y_val, verbose=0)[1]
print("Validation set accuracy: {:.2%}".format(validation_accuracy))

"""*Evaluate the model on the testing set*"""

test_accuracy = best_mlp_model.evaluate(X_test, y_test, verbose=0)[1]
print("Test set accuracy: {:.2%}".format(test_accuracy))

"""Plot training & validation accuracy values

"""

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train'], loc='upper left')

""" Plot training & validation loss values"""

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train'], loc='upper left')

"""Making predictions using a trained MLP (Multi-Layer Perceptron) model on a test
dataset. The predicted values are then converted to binary values by comparing them to a threshold of 0.5. Values greater than 0.5 are considered as 1 (positive class) and values less than or equal to 0.5 are considered as 0 (negative class).
"""

y_pred = best_mlp_model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)

"""*Display classification report*"""

classification_rep = classification_report(y_test, y_pred_binary)
print("Classification Report:\n", classification_rep)

"""Best parameters and accuracys"""

print("Best parameters found: ", grid_result.best_params_)
print("Best accuracy found(Training): {:.2%}".format(grid_result.best_score_))
print("Validation set accuracy: {:.2%}".format(validation_accuracy))
print("Test set accuracy: {:.2%}".format(test_accuracy))

"""# Calculating AUC score and Accuracy

Evaluating the model on the validation set
"""

validation_preds = best_mlp_model.predict(X_val)
validation_accuracy = best_mlp_model.evaluate(X_val, y_val, verbose=0)[1]
validation_auc = roc_auc_score(y_val, validation_preds)

"""Printing Accuracy and AUC score"""

print("Validation set accuracy: {:.2%}".format(validation_accuracy))
print("Validation set AUC: {:.4f}".format(validation_auc))

"""Training and testing the model"""

best_mlp_model.fit(X_train, y_train, epochs=grid_result.best_params_['epochs'], batch_size=grid_result.best_params_['batch_size'], verbose=2)

test_preds = best_mlp_model.predict(X_test)
test_accuracy = best_mlp_model.evaluate(X_test, y_test, verbose=0)[1]
test_auc = roc_auc_score(y_test, test_preds)

"""Printing results for Test set AUC and Accuracy"""

print("Test set accuracy: {:.2%}".format(test_accuracy))
print("Test set AUC: {:.4f}".format(test_auc))

y_pred = grid_search.best_estimator_.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy: {:.4f}".format(accuracy))

y_pred_prob = grid_search.best_estimator_.predict_proba(X_test)
auc_score = roc_auc_score(y_test, y_pred_prob[:, 1])
print("AUC Score: {:.4f}".format(auc_score))

""" Train and testing optimized model"""

optimized_model = grid_search.best_estimator_
optimized_model.fit(X_train, y_train)

"""Printing the Accuracy and AUC of the optimized model"""

y_pred_optimized = optimized_model.predict(X_test)
accuracy_optimized = accuracy_score(y_test, y_pred_optimized)
print("Optimized Model Test Accuracy: {:.4f}".format(accuracy_optimized))


y_pred_prob_optimized = optimized_model.predict_proba(X_test)
auc_score_optimized = roc_auc_score(y_test, y_pred_prob_optimized[:, 1])
print("Optimized Model AUC Score: {:.4f}".format(auc_score_optimized))

"""Optimized results"""

print("Optimized Model Test Accuracy: {:.4f}".format(accuracy_optimized))
print("Optimized Model AUC Score: {:.4f}".format(auc_score_optimized))

"""# Saving Model and Scaler"""

import joblib

# After training and optimizing your model
joblib.dump(optimized_model, 'optimized_model.pkl')

# Save the scaler
import joblib
joblib.dump(scaler, 'scaler.pkl')

